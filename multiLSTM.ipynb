{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a619c7",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c1a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns \n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "# Importing the libraries for evaluation\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score)\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f707f",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a526d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    X_train_load = np.loadtxt('data\\X_train_reshaped_multi.csv', delimiter=',')\n",
    "    X_train_scaled = np.reshape(X_train_load, (X_train_load.shape[0], X_train_load.shape[1], 1))   \n",
    "    X_test_load = np.loadtxt('data\\X_test_reshaped_multi.csv', delimiter=',')\n",
    "    X_test_scaled = np.reshape(X_test_load, (X_test_load.shape[0], X_test_load.shape[1], 1))  \n",
    "    y_train_scaled = np.loadtxt('data\\y_train_reshaped_multi.csv', delimiter=',')\n",
    "    y_test_scaled = np.loadtxt('data\\y_test_reshaped_multi.csv', delimiter=',')\n",
    "    X_val_load = np.loadtxt('data\\X_val_reshaped_multi.csv', delimiter=',')\n",
    "    X_val_scaled = np.reshape(X_val_load, (X_val_load.shape[0], X_val_load.shape[1], 1))\n",
    "    y_val_scaled = np.loadtxt('data\\y_val_reshaped_multi.csv', delimiter=',')\n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, X_val_scaled, y_val_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d00ae3",
   "metadata": {},
   "source": [
    "# Creating the LSTM model for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e790d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train_scaled):\n",
    "    model = Sequential() \n",
    "    # Adding the first LSTM layer and Dropout regularization\n",
    "    model.add(LSTM(units= 76, return_sequences= True, input_shape=  ( X_train_scaled.shape[1], 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Adding the second LSTM layer and Dropout regularization\n",
    "    model.add(LSTM(units= 76, return_sequences= True))\n",
    "    model.add(Dropout(0.2))   \n",
    "    # Adding the third LSTM layer and Dropout regularization\n",
    "    model.add(LSTM(units= 76, return_sequences= True))\n",
    "    model.add(Dropout(0.2))   \n",
    "    # Adding the fourth LSTM layer and Dropout regularization\n",
    "    model.add(LSTM(units= 76))\n",
    "    model.add(Dropout(0.2))    \n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units= 15))\n",
    "    model.add(Activation('softmax'))   \n",
    "    opt = Adam(lr=0.00002)    \n",
    "    # Compiling the LSTM\n",
    "    model.compile(optimizer= opt, loss= 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af45d58",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ac2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled):\n",
    "    earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\",\n",
    "    \t\t\t\t\t\t\t\t\t\tmode =\"min\", patience = 5,\n",
    "    \t\t\t\t\t\t\t\t\t\trestore_best_weights = True)  \n",
    "    hist = model.fit(X_train_scaled, y_train_scaled, batch_size = 1024, epochs = 40, validation_data =(X_val_scaled, y_val_scaled), callbacks = earlystopping)    \n",
    "    fin_epoch = earlystopping.stopped_epoch\n",
    "    return(hist, fin_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b4114",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff042b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test_scaled, y_test_scaled, model, hist):\n",
    "    # Predicting values\n",
    "    y_pred = model.predict_classes(X_test_scaled)\n",
    "    n_values = np.max(y_pred) + 1\n",
    "    y_prednew = np.eye(n_values)[y_pred]\n",
    "    y_prednew = np.reshape(y_prednew, (y_prednew.shape[0], -1))\n",
    "    y_testnew = np.where(y_test_scaled==1)[1]\n",
    "    y_prednew2 = model.predict(X_test_scaled)\n",
    "    # Calculating the performance metrics\n",
    "    training_loss = hist.history['loss']\n",
    "    training_acc = hist.history['accuracy']\n",
    "    loss, accuracy = model.evaluate(X_test_scaled, y_test_scaled)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_testnew, y_pred)\n",
    "    gmean_score = geometric_mean_score(y_testnew, y_pred)\n",
    "    recall = recall_score(y_test_scaled, y_prednew , average=\"weighted\")\n",
    "    precision = precision_score(y_test_scaled, y_prednew , average=\"weighted\")\n",
    "    f1 = f1_score(y_test_scaled, y_prednew, average=\"weighted\") \n",
    "    print(\"Training Loss:\", training_loss)\n",
    "    print(\"Training Accuracy:\", training_acc)\n",
    "    print(\"Overall Accuracy:\", accuracy)\n",
    "    print(\"Overall Loss:\", loss)\n",
    "    print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "    print(\"Geometric Mean:\", gmean_score)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    # Multiclass Confusion Matrix\n",
    "    multi_cm = multilabel_confusion_matrix(y_test_scaled, y_prednew)\n",
    "    return(y_pred, y_prednew, y_prednew2, multi_cm, training_loss, training_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a459a5c",
   "metadata": {},
   "source": [
    "# Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fe0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Accuracy & Loss vs. Epochs\n",
    "def plot_acc_loss(fin_epoch, training_loss, training_acc):\n",
    "    if fin_epoch > 0:     \n",
    "        epoch = fin_epoch\n",
    "    else:\n",
    "        epoch = 40\n",
    "    xc = range(epoch)\n",
    "    plt.figure(1,figsize=(15,epoch)) \n",
    "    plt.plot(xc,training_loss)\n",
    "    plt.xlabel('No. of Epochs') \n",
    "    plt.ylabel('loss') \n",
    "    plt.title('Training Loss') \n",
    "    plt.grid(True) \n",
    "    plt.legend(['Train'])    \n",
    "    plt.figure(2,figsize=(15,epoch)) \n",
    "    plt.plot(xc,training_acc) \n",
    "    plt.xlabel('No. of Epochs') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.title('Training Accuracy') \n",
    "    plt.grid(True) \n",
    "    plt.legend(['Train'],loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c6e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix wrt one-vs-rest\n",
    "def calc_cm(multi_cm, axes, label, class_names, fontsize=25):\n",
    "    df_cm = pd.DataFrame(\n",
    "        multi_cm, index=class_names, columns=class_names)\n",
    "    try:\n",
    "        sns.set(font_scale=2.2)\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes, cmap=\"Blues\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"CM values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_ylabel('True label')\n",
    "    axes.set_xlabel('Predicted label')\n",
    "    axes.set_title(\"CM for the class - \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffdd988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "def plot_roc_auc(y_test_scaled, y_prednew2, class_labels):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = len(class_labels)\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_scaled[:, i], y_prednew2[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_scaled.ravel(), y_prednew2.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])  \n",
    "    # Compute macro-average value for ROC curve and ROC area \n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))    \n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes  \n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    # Plot all ROC curves\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, 15))\n",
    "    for i,color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], \n",
    "                 tpr[i], \n",
    "                 color=color,\n",
    "                 lw=1.5,\n",
    "                 label=\"{}, AUC={:.3f}\".format(class_labels[i], roc_auc[i]))\n",
    "    plt.plot([0,1], [0,1], color='orange', linestyle='--') \n",
    "    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "    plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "    plt.legend(prop={'size':13}, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e1d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curve\n",
    "def plot_pr_auc(y_test_scaled, y_prednew, class_labels):\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "    n_classes = len(class_labels)\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, 15))\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test_scaled[:, i], y_prednew[:, i])\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "    for i,color in zip(range(n_classes), colors):\n",
    "        plt.plot(recall[i], \n",
    "                 precision[i], \n",
    "                 color=color,\n",
    "                 lw=1.5,\n",
    "                 label=\"{}, AUC={:.3f}\".format(class_labels[i], pr_auc[i]))\n",
    "    plt.plot([0,1], [0.5,0.5], color='orange', linestyle='--')\n",
    "    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.xlabel(\"Recall Rate\", fontsize=15)\n",
    "    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "    plt.ylabel(\"Precision Rate\", fontsize=15)\n",
    "    plt.title('Precision Recall Curve', fontweight='bold', fontsize=15)\n",
    "    plt.legend(prop={'size':13}, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    class_labels = [\"Benign\",\n",
    "                    \"Bot\",\n",
    "                    \"Brute Force -Web\",\n",
    "                    \"Brute Force -XSS\",\n",
    "                    \"DDOS attack-HOIC\",\n",
    "                    \"DDOS attack-LOIC-UDP\",\n",
    "                    \"DDoS attacks-LOIC-HTTP\",\n",
    "                    \"DoS attacks-GoldenEye\",\n",
    "                    \"DoS attacks-Hulk\",\n",
    "                    \"DoS attacks-SlowHTTPTest\",\n",
    "                    \"DoS attacks-Slowloris\",\n",
    "                    \"FTP-BruteForce\",\n",
    "                    \"Infiltration\",\n",
    "                    \"SQL Injection\",\n",
    "                    \"SSH-Bruteforce\"]\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, X_val_scaled, y_val_scaled = load_dataset()    \n",
    "    model = create_model(X_train_scaled)\n",
    "    hist = train_model(model, X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled)\n",
    "    y_pred, y_prednew, y_prednew2, multi_cm, class_report = evaluate_model(X_test_scaled, y_test_scaled, model, hist)\n",
    "    # Plot Classification Report\n",
    "    sns.set(font_scale=0.8)\n",
    "    sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True)\n",
    "    # Plot Confusion Matrix\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    for axes, cfs_matrix, label in zip(ax.flatten(), multi_cm, class_labels):\n",
    "        print_confusion_matrix(cfs_matrix, axes, label, [\"N\", \"Y\"])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    # Plot ROC Curve\n",
    "    plot_roc_auc(y_test_scaled, y_prednew2, class_labels)\n",
    "    # Plot PR Curve\n",
    "    plot_pr_auc(y_test_scaled, y_prednew, class_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
