# -*- coding: utf-8 -*-
"""
Created on Tue Feb  8 09:18:39 2022

@author: Administrator
"""

import os
import glob
import pandas as pd
import numpy as np
import dask.dataframe as dd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

def concat_files():
    file_extension = 'CICFlowMeter.csv'
    
    all_filenames = [i for i in glob.glob(f"*{file_extension}")]
    combined_csv_data = pd.concat([pd.read_csv(f, delimiter=',', encoding='UTF-8') for f in all_filenames])
    combined_csv_data = pd.concat([pd.read_csv(f, delimiter=',', encoding='UTF-8') for f in all_filenames], ignore_index=True, verify_integrity=True)
    combined_csv_data.drop_duplicates(keep=False, inplace=True)
    combined_csv_data["Timestamp"] = pd.to_datetime(combined_csv_data["Timestamp"])
    combined_csv_data.sort_values(by=['Timestamp'], inplace=True)
    combined_csv_data = combined_csv_data.drop(["Flow ID", "Src IP", "Src Port", "Dst IP", 'Timestamp', 'Bwd PSH Flags', 'Bwd URG Flags'], axis=1)
    return(combined_csv_data)
    
# Read csv file with chunks for memory save
def load_dataset():
    pd.set_option('mode.use_inf_as_na', True) 
    dataset = dd.read_csv('D:\Dataset\Combined Datasets\combined_csv_data_all.csv', dtype='object').compute()
    # Importing the dataset
    # Taking care of missing data
    dataset = dataset.replace([np.inf, -np.inf], np.nan)
    #For Binary Classification
    #dataset.loc[dataset['Label'] != 'Benign', 'Label'] = 1
    #dataset.loc[dataset['Label'] == 'Benign', 'Label'] = 0
    return(dataset)

def norm_dataset(dataset):
    dataset.iloc[:,15:17] = dataset.iloc[:,15:17].replace('inf', 'nan')
    dataset.iloc[:,15:17] = dataset.iloc[:,15:17].replace('Infinity', 'nan')
    X = dataset.iloc[:, :-1].values
    y = dataset.iloc[:, -1].values
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
    imputer.fit(X[:,15:17])
    X[:, 15:17] = imputer.transform(X[:, 15:17]) 
    y = y.reshape(-1,1)
    onehotencoder1 = OneHotEncoder()
    y = onehotencoder1.fit_transform(y).toarray()
    return(X, y)

def split_dataset(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1) # 
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) 
    #Feature Scaling
    x_scaler = MinMaxScaler(feature_range = (0,1))
    y_scaler = MinMaxScaler(feature_range = (0,1))
    y_train = np.reshape(y_train, (y_train.shape[0], 1)) 
    y_test = np.reshape(y_test, (y_test.shape[0], 1))
    y_val = np.reshape(y_val, (y_val.shape[0], 1))
    X_train_scaled = x_scaler.fit_transform(X_train)
    y_train_scaled = y_scaler.fit_transform(y_train)
    X_train_scaled = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
    X_test_scaled = x_scaler.transform(X_test)
    y_test_scaled = y_scaler.transform(y_test)
    X_test_scaled = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1))
    X_val_scaled = x_scaler.transform(X_val)
    y_val_scaled = y_scaler.transform(y_val)
    X_val_scaled = np.reshape(X_val_scaled, (X_val_scaled.shape[0], X_val_scaled.shape[1], 1))
    return(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, X_val_scaled, y_val_scaled)
    
if __name__=="__main__": 
    dataset = concat_files()
    X, y = norm_dataset(dataset)
    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, X_val_scaled, y_val_scaled = split_dataset(X, y)
    
    
